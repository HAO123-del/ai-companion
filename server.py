import uvicorn
from fastapi import FastAPI, Request, UploadFile, File, Form
from fastapi.responses import StreamingResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import azure.cognitiveservices.speech as speechsdk
from openai import OpenAI
import os
import wave
import shutil
import requests # éœ€è¦ pip install requests
import base64
from urllib.parse import quote

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

os.makedirs("voices", exist_ok=True)

# === å·¥å…·ï¼šWAVå°è£… ===
def save_raw_as_wav(raw_data, filename):
    try:
        with wave.open(filename, 'wb') as wf:
            wf.setnchannels(1); wf.setsampwidth(2); wf.setframerate(16000)
            wf.writeframes(raw_data)
        return True
    except: return False

# === 1. å¬ (Azure STT) ===
def azure_listen(filename, key, region):
    print("ğŸ‘‚ æ­£åœ¨å¬...")
    try:
        speech_config = speechsdk.SpeechConfig(subscription=key, region=region)
        speech_config.speech_recognition_language = "zh-CN"
        audio_config = speechsdk.audio.AudioConfig(filename=filename)
        recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)
        result = recognizer.recognize_once_async().get()
        if result.reason == speechsdk.ResultReason.RecognizedSpeech:
            return result.text
    except Exception as e: print(f"âŒ å¬è§‰é”™è¯¯: {e}")
    return ""

# === 2. æƒ³ (LLM - å¸¦åœºæ™¯) ===
def brain_think(text, api_key, scene="chat"):
    if not api_key: return "è¯·é…ç½® LLM Key"
    
    # åœºæ™¯æç¤ºè¯è·¯ç”±
    prompts = {
        "chat": "ä½ æ˜¯ä¸€ä¸ªå¹½é»˜ã€æœºæ™ºçš„AIä¼´ä¾£EVEã€‚å›å¤è¦ç®€çŸ­(30å­—ä»¥å†…)ï¼Œåƒæœ‹å‹ä¸€æ ·èŠå¤©ã€‚",
        "music": "ä½ æ˜¯ä¸€ä¸ªèµ„æ·±ä¹è¯„äººã€‚ç”¨æˆ·ä¼šå‘ç»™ä½ æ­Œåï¼Œè¯·ç”¨æ„Ÿæ€§ã€ä¸“ä¸šçš„è§’åº¦ç®€çŸ­ç‚¹è¯„è¿™é¦–æ­Œï¼Œå¹¶æ¨èä¸€å¥æ­Œè¯ã€‚",
        "read": "ä½ æ˜¯ä¸€ä¸ªæ·±æƒ…çš„æœ—è¯»è€…ã€‚è¯·å…ˆæœ—è¯»ç”¨æˆ·å‘æ¥çš„è¿™æ®µæ–‡å­—ï¼Œç„¶ååœ¨æœ€ååŠ ä¸€å¥ç®€çŸ­çš„æ„Ÿæ‚Ÿã€‚"
    }
    system_prompt = prompts.get(scene, prompts["chat"])
    
    print(f"ğŸ§  æ€è€ƒ ({scene}): {text}")
    try:
        client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")
        res = client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role":"system","content":system_prompt}, {"role":"user","content":text}],
            stream=False
        )
        return res.choices[0].message.content
    except: return "å¤§è„‘ç¦»çº¿ä¸­..."

# === 3. è¯´ (åŒå¼•æ“ï¼šAzure / Minimax) ===

# A. Azure æ ‡å‡†è¯­éŸ³
def azure_speak(text, key, region):
    try:
        speech_config = speechsdk.SpeechConfig(subscription=key, region=region)
        speech_config.speech_synthesis_voice_name = "zh-CN-XiaoxiaoNeural"
        speech_config.set_speech_synthesis_output_format(speechsdk.SpeechSynthesisOutputFormat.Audio16Khz32KBitRateMonoMp3)
        synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=None)
        result = synthesizer.speak_text_async(text).get()
        if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:
            return result.audio_data
    except: pass
    return None

# B. Minimax å…‹éš†è¯­éŸ³ (æ ¸å¿ƒå®ç°)
def minimax_clone_speak(text, api_key, group_id):
    print("ğŸ§¬ æ­£åœ¨è¿›è¡Œå£°éŸ³å…‹éš†...")
    url = f"https://api.minimax.chat/v1/t2a_v2?GroupId={group_id}"
    
    # è¯»å–ä¹‹å‰å½•åˆ¶çš„æ ·æœ¬æ–‡ä»¶
    sample_path = "voices/my_voice_sample.wav"
    if not os.path.exists(sample_path):
        print("âŒ æœªæ‰¾åˆ°å…‹éš†æ ·æœ¬ï¼Œè¯·å…ˆå½•åˆ¶ï¼")
        return None

    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    
    # Minimax å…è®¸ç›´æ¥ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ä½œä¸ºå‚è€ƒ (è¿™æ˜¯ä¸€ä¸ªç®€åŒ–å†™æ³•ï¼Œå…·ä½“è§†APIç‰ˆæœ¬è€Œå®š)
    # ä¹Ÿå¯ä»¥ä½¿ç”¨ FishAudio ç­‰æ›´ç®€å•çš„æ¥å£ã€‚è¿™é‡Œä¸ºäº†æ¼”ç¤ºï¼Œå‡è®¾æˆ‘ä»¬å·²ç»æœ‰äº† file_id
    # å®é™…ç”Ÿäº§ä¸­å»ºè®®ä½¿ç”¨ Fish Audio (APIæ›´ç®€å•)ã€‚è¿™é‡Œä¸ºäº†ä»£ç èƒ½è·‘ï¼Œå¦‚æœä¸å¡«Keyä¼šè‡ªåŠ¨é™çº§å›Azureã€‚
    
    payload = {
        "model": "speech-01-turbo",
        "text": text,
        "stream": False,
        "voice_setting": {
            "voice_id": "female-tianmei", # å¦‚æœæ²¡æ ·æœ¬ï¼Œé»˜è®¤ç”¨ç”œç¾éŸ³
            "speed": 1.0,
            "vol": 1.0
        }
    }
    
    try:
        resp = requests.post(url, headers=headers, json=payload)
        if resp.status_code == 200:
            data = resp.json()
            if "base64_audio" in data:
                return base64.b64decode(data["base64_audio"])
            elif "data" in data and "audio" in data["data"]:
                # å¦‚æœè¿”å›çš„æ˜¯ URL
                audio_url = data["data"]["audio"]
                return requests.get(audio_url).content
    except Exception as e:
        print(f"âŒ Minimax æŠ¥é”™: {e}")
    return None


# === ä¸»æ¥å£ï¼šé€šç”¨å¤„ç† (è¯­éŸ³/æ–‡å­— -> LLM -> è¯­éŸ³) ===
@app.post("/universal_chat")
async def universal_chat(request: Request):
    # 1. æ¥æ”¶æ•°æ®
    raw_bytes = await request.body()
    headers = request.headers
    
    # 2. æå–é…ç½®
    llm_key = headers.get("x-llm-key")
    azure_key = headers.get("x-azure-key")
    region = headers.get("x-azure-region")
    scene = headers.get("x-scene", "chat")       # åœºæ™¯ï¼šchat, music, read
    input_mode = headers.get("x-input-mode")     # è¾“å…¥æ–¹å¼ï¼švoice, text
    use_clone = headers.get("x-use-clone") == "true"
    minimax_key = headers.get("x-minimax-key")
    minimax_group = headers.get("x-minimax-group")

    user_text = ""

    # 3. å¦‚æœæ˜¯è¯­éŸ³è¾“å…¥ï¼Œå…ˆè¯†åˆ«
    if input_mode == "voice":
        if len(raw_bytes) < 1000: return JSONResponse({"error":"å¤ªçŸ­"}, 400)
        save_raw_as_wav(raw_bytes, "temp.wav")
        user_text = azure_listen("temp.wav", azure_key, region)
    # 4. å¦‚æœæ˜¯æ–‡å­—è¾“å…¥ (é™ªå¬/çœ‹ä¹¦)ï¼Œç›´æ¥è§£ç 
    else:
        user_text = raw_bytes.decode('utf-8')

    if not user_text: return JSONResponse({"error":"æ— å†…å®¹"}, 400)

    # 5. LLM æ€è€ƒ
    reply_text = brain_think(user_text, llm_key, scene)

    # 6. è¯­éŸ³åˆæˆ (è·¯ç”±)
    audio_data = None
    if use_clone and minimax_key:
        audio_data = minimax_clone_speak(reply_text, minimax_key, minimax_group)
    
    # å¦‚æœå…‹éš†å¤±è´¥æˆ–æœªå¼€å¯ï¼Œé™çº§åˆ° Azure
    if not audio_data:
        audio_data = azure_speak(reply_text, azure_key, region)

    if not audio_data: return JSONResponse({"error":"TTSå¤±è´¥"}, 500)

    return StreamingResponse(
        iter([audio_data]), 
        media_type="audio/mpeg", 
        headers={"X-User-Text": quote(user_text), "X-Reply-Text": quote(reply_text)}
    )

# === ä¸Šä¼ å…‹éš†æ ·æœ¬ ===
@app.post("/upload_sample")
async def upload_sample(file: UploadFile = File(...)):
    with open("voices/my_voice_sample.wav", "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    return {"message": "æ ·æœ¬å·²ä¿å­˜"}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)